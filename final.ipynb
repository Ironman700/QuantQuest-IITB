{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97270,"databundleVersionId":11602642,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom functools import reduce\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# --------------------------- Configuration --------------------------- #\ndata_dir = \"/kaggle/input/qrt-quant-quest-iit-bombay-2025/\"\n\nfeatures_list = [\n    'relative_strength_index',\n    'williams_r',\n    'volatility_20',\n    'volatility_60',\n    'trend_1_3',\n    'trend_5_20',\n    'trend_20_60',\n    'average_true_range',\n    'macd',\n    'trix',\n    'commodity_channel_index',\n    'chande_momentum_oscillator',\n    'ichimoku',\n    'know_sure_thing',\n    'ultimate_oscillator',\n    'aroon',\n    'stochastic_oscillator',\n    'on_balance_volume',\n    'ease_of_movement',\n    'chaikin_money_flow',\n    'accumulation_distribution_index',\n    'volume'\n]\n\n# --------------------------- Data Loading --------------------------- #\nfeatures = pd.read_parquet(os.path.join(data_dir, \"features.parquet\"))\nreturns = pd.read_parquet(os.path.join(data_dir, \"returns.parquet\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:55:34.845537Z","iopub.execute_input":"2025-03-30T08:55:34.847047Z","iopub.status.idle":"2025-03-30T08:55:49.050977Z","shell.execute_reply.started":"2025-03-30T08:55:34.846984Z","shell.execute_reply":"2025-03-30T08:55:49.049844Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# --------------------------- Prepare Shifted Features --------------------------- #\nshifted_features = {}\nfor feat in features_list:\n    if feat not in features.columns:\n        raise ValueError(f\"Expected feature '{feat}' not found in features.parquet.\")\n    shifted_features[feat] = features[feat].shift(1)\n\n# --------------------------- Flatten Data --------------------------- #\ndef flatten_df(df, feature_name):\n    \"\"\"\n    Flatten a DataFrame (index: dates, columns: stocks) into long format.\n    Returns a DataFrame with columns: ['date', 'stock', feature_name].\n    \"\"\"\n    df_flat = df.stack().reset_index()\n    df_flat.columns = ['date', 'stock', feature_name]\n    return df_flat\n\n# Flatten each of the 22 shifted features.\ndfs = []\nfor feat in features_list:\n    df_feat = flatten_df(shifted_features[feat], feat)\n    df_feat['date'] = pd.to_datetime(df_feat['date'])\n    dfs.append(df_feat)\n    \n# Flatten returns (target variable) â€” note: returns are not shifted.\ndf_returns = flatten_df(returns, 'return')\ndf_returns['date'] = pd.to_datetime(df_returns['date'])\ndfs.append(df_returns)\n\n# Merge all flattened DataFrames on ['date', 'stock'] using an outer join\ndf_all = reduce(lambda left, right: pd.merge(left, right, on=['date', 'stock'], how='outer'), dfs)\ndf_all['date'] = pd.to_datetime(df_all['date'])\n\nprint(\"Combined data shape:\", df_all.shape)\n# Split data into training (<=2019) and extra (>2019) periods.\ndf_train = df_all[df_all['date'] <= pd.Timestamp(\"2019-12-31\")]\ndf_extra = df_all[df_all['date'] > pd.Timestamp(\"2019-12-31\")]\n\nprint(\"Training data shape:\", df_train.shape)\nprint(\"Extra prediction data shape:\", df_extra.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:55:49.052533Z","iopub.execute_input":"2025-03-30T08:55:49.052832Z","iopub.status.idle":"2025-03-30T08:57:02.163645Z","shell.execute_reply.started":"2025-03-30T08:55:49.052800Z","shell.execute_reply":"2025-03-30T08:57:02.162208Z"}},"outputs":[{"name":"stdout","text":"Combined data shape: (10960686, 25)\nTraining data shape: (8180425, 25)\nExtra prediction data shape: (2780261, 25)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\n\n# ---------------------------\n# Define the Features List\n# ---------------------------\nfeatures_list = [\n    'chaikin_money_flow',          # Top feature based on importance\n    'commodity_channel_index',     # Measures deviation from average price\n    'volatility_20',               # Short-term volatility\n    'know_sure_thing',             # Momentum oscillator\n    'trend_1_3',                   # Short-term trend strength\n    'williams_r'                   # Momentum indicator\n]\n\n# ---------------------------\n# Set Fixed Number of PCA Components\n# ---------------------------\nNUM_COMPONENTS = 6  # Fixed number of PCA components for all stocks\n\n# ---------------------------\n# Per-Stock Modeling and Prediction using LinearRegression with PCA\n# ---------------------------\nstocks = df_train['stock'].unique()\ny_pred_final_list = []\n\nfor stock in stocks:\n    # Get training data for the stock and sort by date.\n    df_stock_train = df_train[df_train['stock'] == stock].sort_values('date')\n    # Get extra data for the stock.\n    df_stock_extra = df_extra[df_extra['stock'] == stock].sort_values('date')\n    \n    # Prepare training predictors and target.\n    X_train = df_stock_train[features_list]\n    y_train = df_stock_train['return']\n    \n    # Debugging: Check if features exist in the data\n    missing_features = [col for col in features_list if col not in X_train.columns]\n    if missing_features:\n        print(f\"Missing features for stock {stock}: {missing_features}\")\n        continue  # Skip this stock if features are missing\n    \n    # Custom imputation using previous day's data (forward fill).\n    X_train = X_train.ffill()  # Forward fill missing values.\n    y_train = y_train.ffill()  # Forward fill missing values.\n    \n    # Replace inf/-inf with NaN and handle remaining NaN values.\n    X_train = X_train.replace([np.inf, -np.inf], np.nan)\n    X_train = X_train.fillna(0)  # Replace remaining NaN values with 0.\n    y_train = y_train.replace([np.inf, -np.inf], np.nan)\n    y_train = y_train.fillna(0)  # Replace remaining NaN values with 0.\n    \n    # Remove constant columns (zero variance).\n    X_train = X_train.loc[:, (X_train != X_train.iloc[0]).any()]  # Keep only non-constant columns.\n    \n    # Debugging: Check if X_train is empty\n    if X_train.empty:\n        # print(f\"X_train is empty for stock {stock}. Skipping...\")\n        continue\n    \n    # Verify that X_train contains no invalid values.\n    assert np.isfinite(X_train.values).all(), f\"X_train contains invalid values for stock {stock}!\"\n    assert np.isfinite(y_train.values).all(), f\"y_train contains invalid values for stock {stock}!\"\n    \n    # Standardize predictors.\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    \n    # Apply PCA for dimensionality reduction with a fixed number of components.\n    pca = PCA(n_components=min(NUM_COMPONENTS, X_train_scaled.shape[1]))  # Ensure n_components <= number of features\n    X_train_pca = pca.fit_transform(X_train_scaled)\n    \n    # Use LinearRegression.\n    model = LinearRegression()\n    model.fit(X_train_pca, y_train)\n    \n    # Predict on training data.\n    y_train_pred = model.predict(X_train_pca)\n    df_stock_train = df_stock_train.copy()\n    df_stock_train['predicted_return'] = y_train_pred\n    \n    # Prepare extra predictors for this stock.\n    X_extra = df_stock_extra[features_list]\n    X_extra = X_extra.ffill()  # Forward fill missing values.\n    X_extra = X_extra.replace([np.inf, -np.inf], np.nan)\n    X_extra = X_extra.fillna(0)  # Replace remaining NaN values with 0.\n    \n    # Align columns of X_extra with X_train\n    X_extra = X_extra[X_train.columns]  # Ensure X_extra has the same columns as X_train\n    \n    # Standardize extra predictors using the same scaler as training.\n    X_extra_scaled = scaler.transform(X_extra)\n    X_extra_pca = pca.transform(X_extra_scaled)  # Use same PCA as training.\n    \n    # Predict on extra data.\n    y_extra_pred = model.predict(X_extra_pca)\n    df_stock_extra = df_stock_extra.copy()\n    df_stock_extra['predicted_return'] = y_extra_pred\n    \n    # Combine training and extra predictions.\n    df_stock_all_pred = pd.concat([df_stock_train[['date', 'stock', 'predicted_return']],\n                                   df_stock_extra[['date', 'stock', 'predicted_return']]])\n    \n    y_pred_final_list.append(df_stock_all_pred)\n\n# Concatenate predictions for all stocks.\ny_pred_final = pd.concat(y_pred_final_list, ignore_index=True)\ny_pred_final['date'] = pd.to_datetime(y_pred_final['date'])\ny_pred_final = y_pred_final.sort_values(['date', 'stock']).reset_index(drop=True)\n\nprint(\"y_pred_final shape (all stocks, full period):\", y_pred_final.shape)\nprint(y_pred_final.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:24:08.863592Z","iopub.execute_input":"2025-03-30T11:24:08.864000Z","iopub.status.idle":"2025-03-30T11:25:18.939712Z","shell.execute_reply.started":"2025-03-30T11:24:08.863962Z","shell.execute_reply":"2025-03-30T11:25:18.938643Z"}},"outputs":[{"name":"stdout","text":"y_pred_final shape (all stocks, full period): (10277856, 3)\n        date  stock  predicted_return\n0 2005-01-03      1          0.000265\n1 2005-01-03      2         -0.000875\n2 2005-01-03      3          0.001375\n3 2005-01-03      4         -0.001547\n4 2005-01-03      5         -0.001256\n","output_type":"stream"}],"execution_count":165},{"cell_type":"code","source":"# ---------------------------\n# Ranking and Universe Filtering\n# ---------------------------\ny_pred_final['rank'] = y_pred_final.groupby('date')['predicted_return'].rank(ascending=False, method='min')\ny_pred_final = y_pred_final.sort_values(['date', 'rank']).reset_index(drop=True)\nprint(\"Ranked predictions sample:\")\nprint(y_pred_final.head())\n\nuniverse = pd.read_parquet(os.path.join(data_dir, \"universe.parquet\"))\ndf_universe = universe.stack().reset_index()\ndf_universe.columns = ['date', 'stock', 'tradable']\ndf_universe['date'] = pd.to_datetime(df_universe['date'])\n\n# Merge predictions with universe data and filter tradable stocks.\ndf_predictions_univ = pd.merge(y_pred_final, df_universe, on=['date', 'stock'], how='left')\ndf_predictions_univ = df_predictions_univ[df_predictions_univ['tradable'] == 1]\ndf_predictions_univ['rank'] = df_predictions_univ.groupby('date')['predicted_return'] \\\n                                                 .rank(ascending=False, method='min')\ndf_predictions_univ = df_predictions_univ.sort_values(['date', 'rank'])\nprint(df_predictions_univ.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:07.902207Z","iopub.execute_input":"2025-03-30T11:29:07.902718Z","iopub.status.idle":"2025-03-30T11:29:22.835549Z","shell.execute_reply.started":"2025-03-30T11:29:07.902685Z","shell.execute_reply":"2025-03-30T11:29:22.834334Z"}},"outputs":[{"name":"stdout","text":"Ranked predictions sample:\n        date  stock  predicted_return  rank\n0 2005-01-03    161          0.994102   1.0\n1 2005-01-03    216          0.023211   2.0\n2 2005-01-03   2068          0.014660   3.0\n3 2005-01-03    383          0.007389   4.0\n4 2005-01-03    625          0.005549   5.0\n            date  stock  predicted_return  rank  tradable\n40640 2005-02-01    161          1.964839   1.0         1\n40643 2005-02-01   1815          0.019317   2.0         1\n40654 2005-02-01    312          0.006122   3.0         1\n40655 2005-02-01   1983          0.006074   4.0         1\n40657 2005-02-01   1077          0.005576   5.0         1\n","output_type":"stream"}],"execution_count":166},{"cell_type":"code","source":"def capture_positions_by_rank(df_ranked):\n    \"\"\"\n    For each trading day, select the top 5 and bottom 5 stocks (if available)\n    from the ranked predictions (df_ranked, which must have a 'predicted_return' column),\n    and assign weights:\n        - Top 5: +0.1 each\n        - Bottom 5: -0.1 each\n        - Others: 0\n    This guarantees:\n        - Maximum individual weight: 0.1\n        - If at least 10 stocks are tradable, then unit capital = sum(|w|) = 1 \n          (5Ã—0.1 + 5Ã—0.1) and dollar neutrality (net sum = 0).\n    If fewer than 10 stocks are tradable, positions are assigned at Â±0.1\n    and unit capital will be less than 1.\n    \"\"\"\n    def process_day(day_df):\n        # Assume day_df already contains only tradable stocks for that day.\n        day_df = day_df.copy().sort_values(by='predicted_return', ascending=False)\n        n = len(day_df)\n        if n >= 10:\n            n_long = 5\n            n_short = 5\n        else:\n            # Split available stocks as evenly as possible between long and short.\n            n_long = n // 2\n            n_short = n - n_long\n        # Create a weight column: default is 0.\n        day_df['weight'] = 0.0\n        \n        # Assign +0.1 to top n_long stocks.\n        if n_long > 0:\n            idx_long = day_df.index[:n_long]\n            day_df.loc[idx_long, 'weight'] = 0.1\n        # Assign -0.1 to bottom n_short stocks.\n        if n_short > 0:\n            idx_short = day_df.index[-n_short:]\n            day_df.loc[idx_short, 'weight'] = -0.1\n        \n        # (Optional) If exactly 10 stocks, then unit capital = 1 and net sum = 0.\n        # If fewer stocks, unit capital will be sum(|w|), which will be < 1.\n        return day_df\n\n    df_positions = df_ranked.groupby('date').apply(process_day).reset_index(drop=True)\n    return df_positions\n#     ('max_abs_weight', lambda x: x.abs().max()),\n#     ('sum_abs_weight', lambda x: x.abs().sum())\n# ])\n# print(daily_checks.sort_values('weight_sum', ascending=False).head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:22.837003Z","iopub.execute_input":"2025-03-30T11:29:22.837313Z","iopub.status.idle":"2025-03-30T11:29:22.844860Z","shell.execute_reply.started":"2025-03-30T11:29:22.837289Z","shell.execute_reply":"2025-03-30T11:29:22.843738Z"}},"outputs":[],"execution_count":167},{"cell_type":"code","source":"df_positions = capture_positions_by_rank(df_predictions_univ)\n\n# Compute daily checks: \n#   - weight_sum: net sum of weights (should be near zero)\n#   - max_abs_weight: maximum absolute weight (should be <= 0.1)\n#   - sum_abs_weight: sum of absolute weights (should be 1)\ndaily_checks = df_positions.groupby('date')['weight'].agg([\n    ('weight_sum', lambda x: np.sum(x)),\n    ('max_abs_weight', lambda x: np.max(np.abs(x))),\n    ('sum_abs_weight', lambda x: np.sum(np.abs(x)))\n])\nprint(daily_checks.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:22.846522Z","iopub.execute_input":"2025-03-30T11:29:22.846793Z","iopub.status.idle":"2025-03-30T11:29:34.745730Z","shell.execute_reply.started":"2025-03-30T11:29:22.846770Z","shell.execute_reply":"2025-03-30T11:29:34.744675Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-167-3f8154eb4fb1>:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_positions = df_ranked.groupby('date').apply(process_day).reset_index(drop=True)\n","output_type":"stream"},{"name":"stdout","text":"            weight_sum  max_abs_weight  sum_abs_weight\ndate                                                  \n2005-02-01         0.0             0.1             1.0\n2005-02-02         0.0             0.1             1.0\n2005-02-03         0.0             0.1             1.0\n2005-02-04         0.0             0.1             1.0\n2005-02-07         0.0             0.1             1.0\n","output_type":"stream"}],"execution_count":168},{"cell_type":"code","source":"df_positions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:34.747232Z","iopub.execute_input":"2025-03-30T11:29:34.747537Z","iopub.status.idle":"2025-03-30T11:29:34.762281Z","shell.execute_reply.started":"2025-03-30T11:29:34.747512Z","shell.execute_reply":"2025-03-30T11:29:34.760998Z"}},"outputs":[{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"              date  stock  predicted_return   rank  tradable  weight\n0       2005-02-01    161          1.964839    1.0         1     0.1\n1       2005-02-01   1815          0.019317    2.0         1     0.1\n2       2005-02-01    312          0.006122    3.0         1     0.1\n3       2005-02-01   1983          0.006074    4.0         1     0.1\n4       2005-02-01   1077          0.005576    5.0         1     0.1\n...            ...    ...               ...    ...       ...     ...\n4995172 2025-02-07    803         -0.014333  960.0         1    -0.1\n4995173 2025-02-07     75         -0.026238  961.0         1    -0.1\n4995174 2025-02-07   2060         -0.026769  962.0         1    -0.1\n4995175 2025-02-07    500         -0.029070  963.0         1    -0.1\n4995176 2025-02-07   1514         -0.170032  964.0         1    -0.1\n\n[4995177 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>stock</th>\n      <th>predicted_return</th>\n      <th>rank</th>\n      <th>tradable</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2005-02-01</td>\n      <td>161</td>\n      <td>1.964839</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2005-02-01</td>\n      <td>1815</td>\n      <td>0.019317</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2005-02-01</td>\n      <td>312</td>\n      <td>0.006122</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2005-02-01</td>\n      <td>1983</td>\n      <td>0.006074</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2005-02-01</td>\n      <td>1077</td>\n      <td>0.005576</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995172</th>\n      <td>2025-02-07</td>\n      <td>803</td>\n      <td>-0.014333</td>\n      <td>960.0</td>\n      <td>1</td>\n      <td>-0.1</td>\n    </tr>\n    <tr>\n      <th>4995173</th>\n      <td>2025-02-07</td>\n      <td>75</td>\n      <td>-0.026238</td>\n      <td>961.0</td>\n      <td>1</td>\n      <td>-0.1</td>\n    </tr>\n    <tr>\n      <th>4995174</th>\n      <td>2025-02-07</td>\n      <td>2060</td>\n      <td>-0.026769</td>\n      <td>962.0</td>\n      <td>1</td>\n      <td>-0.1</td>\n    </tr>\n    <tr>\n      <th>4995175</th>\n      <td>2025-02-07</td>\n      <td>500</td>\n      <td>-0.029070</td>\n      <td>963.0</td>\n      <td>1</td>\n      <td>-0.1</td>\n    </tr>\n    <tr>\n      <th>4995176</th>\n      <td>2025-02-07</td>\n      <td>1514</td>\n      <td>-0.170032</td>\n      <td>964.0</td>\n      <td>1</td>\n      <td>-0.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>4995177 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":169},{"cell_type":"code","source":"# ---------------------------\n# Portfolio Metrics and Backtesting\n# ---------------------------\ndef calculate_metrics(df_positions, returns):\n    \"\"\"\n    Calculates daily portfolio metrics: BookValue, Traded, GrossPnL, and NetPnL.\n    \"\"\"\n    # Pivot df_positions so that each date is a row and each stock is a column.\n    pivot_weights = df_positions.pivot(index='date', columns='stock', values='weight').fillna(0.0)\n    \n    # Ensure alignment with returns.\n    common_dates = pivot_weights.index.intersection(returns.index)\n    pivot_weights = pivot_weights.loc[common_dates].sort_index()\n    returns = returns.loc[common_dates].sort_index()\n    \n    # BookValue: sum of absolute weights.\n    book_value_series = pivot_weights.abs().sum(axis=1)\n    \n    # Traded: sum of absolute differences in weights from previous day.\n    shifted_weights = pivot_weights.shift(1, fill_value=0.0)\n    traded_series = (pivot_weights - shifted_weights).abs().sum(axis=1)\n    \n    # GrossPnL: weighted sum of returns.\n    gross_pnl_series = (pivot_weights * returns).sum(axis=1)\n    \n    # NetPnL: subtract trading costs (0.01% per traded amount).\n    net_pnl_series = gross_pnl_series - (0.0001 * traded_series)\n    \n    metrics_df = pd.DataFrame({\n        'BookValue': book_value_series,\n        'Traded': traded_series,\n        'GrossPnL': gross_pnl_series,\n        'NetPnL': net_pnl_series\n    })\n    \n    # Compute overall turnover.\n    total_traded = traded_series.sum()\n    total_book_value = book_value_series.sum()\n    turnover = (total_traded / total_book_value) * 100.0 if total_book_value != 0 else 0.0\n    \n    return metrics_df, turnover\n\nmetrics_df, turnover = calculate_metrics(df_positions, returns)\nprint(metrics_df.head())\nprint(\"Overall Turnover (%):\", turnover)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:34.763464Z","iopub.execute_input":"2025-03-30T11:29:34.763787Z","iopub.status.idle":"2025-03-30T11:29:37.136604Z","shell.execute_reply.started":"2025-03-30T11:29:34.763757Z","shell.execute_reply":"2025-03-30T11:29:37.135559Z"}},"outputs":[{"name":"stdout","text":"            BookValue  Traded  GrossPnL    NetPnL\n2005-02-01        1.0     1.0  0.004246  0.004146\n2005-02-02        1.0     1.0  0.007627  0.007527\n2005-02-03        1.0     1.2  0.000284  0.000164\n2005-02-04        1.0     0.8 -0.007646 -0.007726\n2005-02-07        1.0     1.2  0.002234  0.002114\nOverall Turnover (%): 103.11051930758988\n","output_type":"stream"}],"execution_count":170},{"cell_type":"code","source":"def calculate_sharpes(metrics_df):\n    \"\"\"\n    Calculates Annualized Gross and Net Sharpe Ratios.\n    \"\"\"\n    gross_pnl = metrics_df['GrossPnL']\n    net_pnl   = metrics_df['NetPnL']\n    ann_factor = np.sqrt(252)\n    \n    gross_mean = gross_pnl.mean()\n    gross_std  = gross_pnl.std(ddof=1)\n    net_mean   = net_pnl.mean()\n    net_std    = net_pnl.std(ddof=1)\n    \n    gross_sharpe = ann_factor * (gross_mean / gross_std) if gross_std != 0 else np.nan\n    net_sharpe   = ann_factor * (net_mean / net_std) if net_std != 0 else np.nan\n    \n    return gross_sharpe, net_sharpe\n\ngross_sharpe, net_sharpe = calculate_sharpes(metrics_df)\nprint(f\"Annualized Gross Sharpe Ratio: {gross_sharpe:.4f}\")\nprint(f\"Annualized Net Sharpe Ratio: {net_sharpe:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:37.137533Z","iopub.execute_input":"2025-03-30T11:29:37.137795Z","iopub.status.idle":"2025-03-30T11:29:37.146691Z","shell.execute_reply.started":"2025-03-30T11:29:37.137774Z","shell.execute_reply":"2025-03-30T11:29:37.145443Z"}},"outputs":[{"name":"stdout","text":"Annualized Gross Sharpe Ratio: 4.2877\nAnnualized Net Sharpe Ratio: 4.1843\n","output_type":"stream"}],"execution_count":171},{"cell_type":"code","source":"# ---------------------------\n# Prepare Final Submission Weights\n# ---------------------------\n# Get the full list of stocks from one of the feature DataFrames.\nall_stocks = features[features_list[0]].columns.tolist()\n# Pivot positions to create a DataFrame with dates as rows and stocks as columns.\ndf_weights = df_positions.pivot(index='date', columns='stock', values='weight')\n# Reindex to include all stocks (fill missing with zero).\ndf_weights = df_weights.reindex(columns=all_stocks, fill_value=0).fillna(0)\nprint(\"Submission weights shape:\", df_weights.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:37.147993Z","iopub.execute_input":"2025-03-30T11:29:37.148407Z","iopub.status.idle":"2025-03-30T11:29:39.100464Z","shell.execute_reply.started":"2025-03-30T11:29:37.148369Z","shell.execute_reply":"2025-03-30T11:29:39.099494Z"}},"outputs":[{"name":"stdout","text":"Submission weights shape: (5038, 2167)\n","output_type":"stream"}],"execution_count":172},{"cell_type":"code","source":"training_dates = sorted(df_train['date'].unique())[:20]\n\n# Create a DataFrame with zero weights for these 20 dates and with the same columns as df_weights.\ndf_zeros = pd.DataFrame(0.0, index=training_dates, columns=df_weights.columns)\n\n# Concatenate the zero-weight DataFrame with the original df_weights.\n# This places the 20 zero-weight days before the existing dates.\ndf_weights = pd.concat([df_zeros, df_weights])\n\n# Sort the DataFrame by date to ensure proper chronological order.\ndf_weights = df_weights.sort_index()\n\ndf_weights.index.name = \"Date\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:29:39.102680Z","iopub.execute_input":"2025-03-30T11:29:39.103021Z","iopub.status.idle":"2025-03-30T11:29:39.295236Z","shell.execute_reply.started":"2025-03-30T11:29:39.102995Z","shell.execute_reply":"2025-03-30T11:29:39.294083Z"}},"outputs":[],"execution_count":173},{"cell_type":"code","source":"df_weights.iloc[:2700, :] = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T10:08:28.823706Z","iopub.execute_input":"2025-03-30T10:08:28.824063Z","iopub.status.idle":"2025-03-30T10:08:28.834805Z","shell.execute_reply.started":"2025-03-30T10:08:28.824037Z","shell.execute_reply":"2025-03-30T10:08:28.833761Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"# Optionally, if required, save the submission file.\ndf_weights.to_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:30:06.689240Z","iopub.execute_input":"2025-03-30T11:30:06.689634Z","iopub.status.idle":"2025-03-30T11:30:13.928485Z","shell.execute_reply.started":"2025-03-30T11:30:06.689607Z","shell.execute_reply":"2025-03-30T11:30:13.927364Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"df_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:30:19.734142Z","iopub.execute_input":"2025-03-30T11:30:19.734660Z","iopub.status.idle":"2025-03-30T11:30:19.788383Z","shell.execute_reply.started":"2025-03-30T11:30:19.734623Z","shell.execute_reply":"2025-03-30T11:30:19.787373Z"}},"outputs":[{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"stock       1     2     3     4     5     6     7     8     9     10    ...  \\\nDate                                                                    ...   \n2005-01-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2005-01-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2005-01-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2005-01-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2005-01-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n2025-02-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2025-02-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2025-02-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2025-02-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n2025-02-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n\nstock       2158  2159  2160  2161  2162  2163  2164  2165  2166  2167  \nDate                                                                    \n2005-01-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2005-01-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2005-01-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2005-01-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2005-01-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n2025-02-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2025-02-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2025-02-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2025-02-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2025-02-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n\n[5058 rows x 2167 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>stock</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>2158</th>\n      <th>2159</th>\n      <th>2160</th>\n      <th>2161</th>\n      <th>2162</th>\n      <th>2163</th>\n      <th>2164</th>\n      <th>2165</th>\n      <th>2166</th>\n      <th>2167</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2005-01-03</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2005-01-04</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2005-01-05</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2005-01-06</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2005-01-07</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2025-02-03</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2025-02-04</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2025-02-05</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2025-02-06</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2025-02-07</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5058 rows Ã— 2167 columns</p>\n</div>"},"metadata":{}}],"execution_count":175},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}