{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:40:57.411994Z",
     "iopub.status.busy": "2025-03-30T04:40:57.411697Z",
     "iopub.status.idle": "2025-03-30T04:41:11.053988Z",
     "shell.execute_reply": "2025-03-30T04:41:11.053241Z",
     "shell.execute_reply.started": "2025-03-30T04:40:57.411965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --------------------------- Configuration --------------------------- #\n",
    "data_dir = \"/home/ironman/Desktop/qrt-quant-quest-iit-bombay-2025/\"\n",
    "\n",
    "features_list = [\n",
    "    'relative_strength_index',\n",
    "    'williams_r',\n",
    "    'volatility_20',\n",
    "    'volatility_60',\n",
    "    'trend_1_3',\n",
    "    'trend_5_20',\n",
    "    'trend_20_60',\n",
    "    'average_true_range',\n",
    "    'macd',\n",
    "    'trix',\n",
    "    'commodity_channel_index',\n",
    "    'chande_momentum_oscillator',\n",
    "    'ichimoku',\n",
    "    'know_sure_thing',\n",
    "    'ultimate_oscillator',\n",
    "    'aroon',\n",
    "    'stochastic_oscillator',\n",
    "    'on_balance_volume',\n",
    "    'ease_of_movement',\n",
    "    'chaikin_money_flow',\n",
    "    'accumulation_distribution_index',\n",
    "    'volume'\n",
    "]\n",
    "\n",
    "# --------------------------- Data Loading --------------------------- #\n",
    "features = pd.read_parquet(os.path.join(data_dir, \"features.parquet\"))\n",
    "returns = pd.read_parquet(os.path.join(data_dir, \"returns.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:41:13.644676Z",
     "iopub.status.busy": "2025-03-30T04:41:13.644421Z",
     "iopub.status.idle": "2025-03-30T04:41:48.758826Z",
     "shell.execute_reply": "2025-03-30T04:41:48.758211Z",
     "shell.execute_reply.started": "2025-03-30T04:41:13.644657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- Prepare Shifted Features --------------------------- #\n",
    "shifted_features = {}\n",
    "for feat in features_list:\n",
    "    if feat not in features.columns:\n",
    "        raise ValueError(f\"Expected feature '{feat}' not found in features.parquet.\")\n",
    "    shifted_features[feat] = features[feat].shift(1)\n",
    "\n",
    "# --------------------------- Flatten Data --------------------------- #\n",
    "def flatten_df(df, feature_name):\n",
    "    \"\"\"\n",
    "    Flatten a DataFrame (index: dates, columns: stocks) into long format.\n",
    "    Returns a DataFrame with columns: ['date', 'stock', feature_name].\n",
    "    \"\"\"\n",
    "    df_flat = df.stack().reset_index()\n",
    "    df_flat.columns = ['date', 'stock', feature_name]\n",
    "    return df_flat\n",
    "\n",
    "# Flatten each of the 22 shifted features.\n",
    "dfs = []\n",
    "for feat in features_list:\n",
    "    df_feat = flatten_df(shifted_features[feat], feat)\n",
    "    df_feat['date'] = pd.to_datetime(df_feat['date'])\n",
    "    dfs.append(df_feat)\n",
    "    \n",
    "# Flatten returns (target variable) â€” note: returns are not shifted.\n",
    "df_returns = flatten_df(returns, 'return')\n",
    "df_returns['date'] = pd.to_datetime(df_returns['date'])\n",
    "dfs.append(df_returns)\n",
    "\n",
    "# Merge all flattened DataFrames on ['date', 'stock'] using an outer join\n",
    "df_all = reduce(lambda left, right: pd.merge(left, right, on=['date', 'stock'], how='outer'), dfs)\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "print(\"Combined data shape:\", df_all.shape)\n",
    "# Split data into training (<=2019) and extra (>2019) periods.\n",
    "df_train = df_all[df_all['date'] <= pd.Timestamp(\"2019-12-31\")]\n",
    "df_extra = df_all[df_all['date'] > pd.Timestamp(\"2019-12-31\")]\n",
    "\n",
    "print(\"Training data shape:\", df_train.shape)\n",
    "print(\"Extra prediction data shape:\", df_extra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:41:53.253295Z",
     "iopub.status.busy": "2025-03-30T04:41:53.253019Z",
     "iopub.status.idle": "2025-03-30T04:43:43.903145Z",
     "shell.execute_reply": "2025-03-30T04:43:43.902450Z",
     "shell.execute_reply.started": "2025-03-30T04:41:53.253275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_final shape (all stocks, full period): (10960686, 3)\n",
      "        date  stock  predicted_return\n",
      "0 2005-01-03      1          0.000643\n",
      "1 2005-01-03      2          0.000163\n",
      "2 2005-01-03      3          0.000643\n",
      "3 2005-01-03      4          0.000262\n",
      "4 2005-01-03      5          0.000666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Determine Optimal Number of PCA Components Using Elbow Curve\n",
    "# ---------------------------\n",
    "def determine_pca_components(X_train_scaled, max_components=10):\n",
    "    \"\"\"\n",
    "    Determine the optimal number of PCA components using the elbow method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_scaled : np.ndarray\n",
    "        Scaled training data.\n",
    "    max_components : int\n",
    "        Maximum number of PCA components to consider.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Optimal number of PCA components.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(X_train_scaled)\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, max_components + 1), explained_variance, marker='o', linestyle='--')\n",
    "    plt.title('Explained Variance by Number of PCA Components')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Choose the number of components where the curve starts to flatten\n",
    "    optimal_components = np.argmax(explained_variance >= 0.95) + 1  # 95% variance threshold\n",
    "    return max(5, min(optimal_components, max_components))  # Ensure it's between 5 and max_components\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Per-Stock Modeling and Prediction using DecisionTreeRegressor with PCA and Regularization\n",
    "# ---------------------------\n",
    "stocks = df_train['stock'].unique()\n",
    "y_pred_final_list = []\n",
    "\n",
    "for stock in stocks:\n",
    "    # Get training data for the stock and sort by date.\n",
    "    df_stock_train = df_train[df_train['stock'] == stock].sort_values('date')\n",
    "    # Get extra data for the stock.\n",
    "    df_stock_extra = df_extra[df_extra['stock'] == stock].sort_values('date')\n",
    "    \n",
    "    # Prepare training predictors and target.\n",
    "    X_train = df_stock_train[features_list]\n",
    "    y_train = df_stock_train['return']\n",
    "    \n",
    "    # Custom imputation using previous day's data (forward fill).\n",
    "    X_train = X_train.ffill()  # Forward fill missing values.\n",
    "    y_train = y_train.ffill()  # Forward fill missing values.\n",
    "    \n",
    "    # Replace inf/-inf with NaN and handle remaining NaN values.\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "    X_train = X_train.fillna(0)  # Replace remaining NaN values with 0.\n",
    "    y_train = y_train.replace([np.inf, -np.inf], np.nan)\n",
    "    y_train = y_train.fillna(0)  # Replace remaining NaN values with 0.\n",
    "    \n",
    "    # Verify that X_train contains no invalid values.\n",
    "    assert np.isfinite(X_train.values).all(), \"X_train contains invalid values!\"\n",
    "    assert np.isfinite(y_train.values).all(), \"y_train contains invalid values!\"\n",
    "    \n",
    "    # Standardize predictors.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Determine the optimal number of PCA components.\n",
    "    num_components = determine_pca_components(X_train_scaled, max_components=10)\n",
    "    print(f\"Optimal number of PCA components for stock {stock}: {num_components}\")\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction.\n",
    "    pca = PCA(n_components=num_components)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    \n",
    "    # Use DecisionTreeRegressor with regularization.\n",
    "    model = DecisionTreeRegressor(\n",
    "        max_depth=5,               # Limit the depth of the tree\n",
    "        min_samples_split=10,      # Minimum samples required to split an internal node\n",
    "        min_samples_leaf=5,        # Minimum samples required to be at a leaf node\n",
    "        max_features='sqrt',       # Use a subset of features for splitting\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Predict on training data.\n",
    "    y_train_pred = model.predict(X_train_pca)\n",
    "    df_stock_train = df_stock_train.copy()\n",
    "    df_stock_train['predicted_return'] = y_train_pred\n",
    "    \n",
    "    # Prepare extra predictors for this stock.\n",
    "    X_extra = df_stock_extra[features_list]\n",
    "    X_extra = X_extra.ffill()  # Forward fill missing values.\n",
    "    X_extra = X_extra.replace([np.inf, -np.inf], np.nan)\n",
    "    X_extra = X_extra.fillna(0)  # Replace remaining NaN values with 0.\n",
    "    X_extra_scaled = scaler.transform(X_extra)  # Use same scaler as training.\n",
    "    X_extra_pca = pca.transform(X_extra_scaled)  # Use same PCA as training.\n",
    "    \n",
    "    # Predict on extra data.\n",
    "    y_extra_pred = model.predict(X_extra_pca)\n",
    "    df_stock_extra = df_stock_extra.copy()\n",
    "    df_stock_extra['predicted_return'] = y_extra_pred\n",
    "    \n",
    "    # Combine training and extra predictions.\n",
    "    df_stock_all_pred = pd.concat([df_stock_train[['date', 'stock', 'predicted_return']],\n",
    "                                   df_stock_extra[['date', 'stock', 'predicted_return']]])\n",
    "    \n",
    "    y_pred_final_list.append(df_stock_all_pred)\n",
    "\n",
    "# Concatenate predictions for all stocks.\n",
    "y_pred_final = pd.concat(y_pred_final_list, ignore_index=True)\n",
    "y_pred_final['date'] = pd.to_datetime(y_pred_final['date'])\n",
    "y_pred_final = y_pred_final.sort_values(['date', 'stock']).reset_index(drop=True)\n",
    "\n",
    "print(\"y_pred_final shape (all stocks, full period):\", y_pred_final.shape)\n",
    "print(y_pred_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:00:13.729769Z",
     "iopub.status.busy": "2025-03-30T05:00:13.729495Z",
     "iopub.status.idle": "2025-03-30T05:00:19.401826Z",
     "shell.execute_reply": "2025-03-30T05:00:19.401041Z",
     "shell.execute_reply.started": "2025-03-30T05:00:13.729747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked predictions sample:\n",
      "        date  stock  predicted_return  rank\n",
      "0 2005-01-03    296          0.008871   1.0\n",
      "1 2005-01-03    222          0.004981   2.0\n",
      "2 2005-01-03    616          0.004169   3.0\n",
      "3 2005-01-03    191          0.003556   4.0\n",
      "4 2005-01-03   1269          0.003273   5.0\n",
      "            date  stock  predicted_return  rank  tradable\n",
      "43344 2005-02-01   1699          0.040580   1.0         1\n",
      "43346 2005-02-01   1676          0.027538   2.0         1\n",
      "43347 2005-02-01   1556          0.017221   3.0         1\n",
      "43348 2005-02-01   1719          0.013154   4.0         1\n",
      "43351 2005-02-01   1788          0.009610   5.0         1\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Ranking and Universe Filtering\n",
    "# ---------------------------\n",
    "y_pred_final['rank'] = y_pred_final.groupby('date')['predicted_return'].rank(ascending=False, method='min')\n",
    "y_pred_final = y_pred_final.sort_values(['date', 'rank']).reset_index(drop=True)\n",
    "print(\"Ranked predictions sample:\")\n",
    "print(y_pred_final.head())\n",
    "\n",
    "universe = pd.read_parquet(os.path.join(data_dir, \"universe.parquet\"))\n",
    "df_universe = universe.stack().reset_index()\n",
    "df_universe.columns = ['date', 'stock', 'tradable']\n",
    "df_universe['date'] = pd.to_datetime(df_universe['date'])\n",
    "\n",
    "# Merge predictions with universe data and filter tradable stocks.\n",
    "df_predictions_univ = pd.merge(y_pred_final, df_universe, on=['date', 'stock'], how='left')\n",
    "df_predictions_univ = df_predictions_univ[df_predictions_univ['tradable'] == 1]\n",
    "df_predictions_univ['rank'] = df_predictions_univ.groupby('date')['predicted_return'] \\\n",
    "                                                 .rank(ascending=False, method='min')\n",
    "df_predictions_univ = df_predictions_univ.sort_values(['date', 'rank'])\n",
    "print(df_predictions_univ.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:04:43.260524Z",
     "iopub.status.busy": "2025-03-30T05:04:43.260223Z",
     "iopub.status.idle": "2025-03-30T05:04:43.265693Z",
     "shell.execute_reply": "2025-03-30T05:04:43.265051Z",
     "shell.execute_reply.started": "2025-03-30T05:04:43.260501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def capture_positions_by_rank(df_ranked):\n",
    "    \"\"\"\n",
    "    For each trading day, select the top 5 and bottom 5 stocks (if available)\n",
    "    from the ranked predictions (df_ranked, which must have a 'predicted_return' column),\n",
    "    and assign weights:\n",
    "        - Top 5: +0.1 each\n",
    "        - Bottom 5: -0.1 each\n",
    "        - Others: 0\n",
    "    This guarantees:\n",
    "        - Maximum individual weight: 0.1\n",
    "        - If at least 10 stocks are tradable, then unit capital = sum(|w|) = 1 \n",
    "          (5Ã—0.1 + 5Ã—0.1) and dollar neutrality (net sum = 0).\n",
    "    If fewer than 10 stocks are tradable, positions are assigned at Â±0.1\n",
    "    and unit capital will be less than 1.\n",
    "    \"\"\"\n",
    "    def process_day(day_df):\n",
    "        # Assume day_df already contains only tradable stocks for that day.\n",
    "        day_df = day_df.copy().sort_values(by='predicted_return', ascending=False)\n",
    "        n = len(day_df)\n",
    "        if n >= 10:\n",
    "            n_long = 5\n",
    "            n_short = 5\n",
    "        else:\n",
    "            # Split available stocks as evenly as possible between long and short.\n",
    "            n_long = n // 2\n",
    "            n_short = n - n_long\n",
    "        # Create a weight column: default is 0.\n",
    "        day_df['weight'] = 0.0\n",
    "        \n",
    "        # Assign +0.1 to top n_long stocks.\n",
    "        if n_long > 0:\n",
    "            idx_long = day_df.index[:n_long]\n",
    "            day_df.loc[idx_long, 'weight'] = 0.1\n",
    "        # Assign -0.1 to bottom n_short stocks.\n",
    "        if n_short > 0:\n",
    "            idx_short = day_df.index[-n_short:]\n",
    "            day_df.loc[idx_short, 'weight'] = -0.1\n",
    "        \n",
    "        # (Optional) If exactly 10 stocks, then unit capital = 1 and net sum = 0.\n",
    "        # If fewer stocks, unit capital will be sum(|w|), which will be < 1.\n",
    "        return day_df\n",
    "\n",
    "    df_positions = df_ranked.groupby('date').apply(process_day).reset_index(drop=True)\n",
    "    return df_positions\n",
    "#     ('max_abs_weight', lambda x: x.abs().max()),\n",
    "#     ('sum_abs_weight', lambda x: x.abs().sum())\n",
    "# ])\n",
    "# print(daily_checks.sort_values('weight_sum', ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:04:59.983056Z",
     "iopub.status.busy": "2025-03-30T05:04:59.982700Z",
     "iopub.status.idle": "2025-03-30T05:05:08.010647Z",
     "shell.execute_reply": "2025-03-30T05:05:08.010011Z",
     "shell.execute_reply.started": "2025-03-30T05:04:59.983032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f886ebf223fa>:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_positions = df_ranked.groupby('date').apply(process_day).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            weight_sum  max_abs_weight  sum_abs_weight\n",
      "date                                                  \n",
      "2005-02-01         0.0             0.1             1.0\n",
      "2005-02-02         0.0             0.1             1.0\n",
      "2005-02-03         0.0             0.1             1.0\n",
      "2005-02-04         0.0             0.1             1.0\n",
      "2005-02-07         0.0             0.1             1.0\n"
     ]
    }
   ],
   "source": [
    "df_positions = capture_positions_by_rank(df_predictions_univ)\n",
    "\n",
    "# Compute daily checks: \n",
    "#   - weight_sum: net sum of weights (should be near zero)\n",
    "#   - max_abs_weight: maximum absolute weight (should be <= 0.1)\n",
    "#   - sum_abs_weight: sum of absolute weights (should be 1)\n",
    "daily_checks = df_positions.groupby('date')['weight'].agg([\n",
    "    ('weight_sum', lambda x: np.sum(x)),\n",
    "    ('max_abs_weight', lambda x: np.max(np.abs(x))),\n",
    "    ('sum_abs_weight', lambda x: np.sum(np.abs(x)))\n",
    "])\n",
    "print(daily_checks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:05:18.546252Z",
     "iopub.status.busy": "2025-03-30T05:05:18.545978Z",
     "iopub.status.idle": "2025-03-30T05:05:18.556067Z",
     "shell.execute_reply": "2025-03-30T05:05:18.555375Z",
     "shell.execute_reply.started": "2025-03-30T05:05:18.546229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>predicted_return</th>\n",
       "      <th>rank</th>\n",
       "      <th>tradable</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>1699</td>\n",
       "      <td>0.040580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>1676</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>1556</td>\n",
       "      <td>0.017221</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>1719</td>\n",
       "      <td>0.013154</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-02-01</td>\n",
       "      <td>1788</td>\n",
       "      <td>0.009610</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037995</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>439</td>\n",
       "      <td>-0.215124</td>\n",
       "      <td>996.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037996</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>1980</td>\n",
       "      <td>-0.256944</td>\n",
       "      <td>997.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037997</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>134</td>\n",
       "      <td>-0.264694</td>\n",
       "      <td>998.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037998</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>847</td>\n",
       "      <td>-0.272258</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037999</th>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>502</td>\n",
       "      <td>-0.361338</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5038000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  stock  predicted_return    rank  tradable  weight\n",
       "0       2005-02-01   1699          0.040580     1.0         1     0.1\n",
       "1       2005-02-01   1676          0.027538     2.0         1     0.1\n",
       "2       2005-02-01   1556          0.017221     3.0         1     0.1\n",
       "3       2005-02-01   1719          0.013154     4.0         1     0.1\n",
       "4       2005-02-01   1788          0.009610     5.0         1     0.1\n",
       "...            ...    ...               ...     ...       ...     ...\n",
       "5037995 2025-02-07    439         -0.215124   996.0         1    -0.1\n",
       "5037996 2025-02-07   1980         -0.256944   997.0         1    -0.1\n",
       "5037997 2025-02-07    134         -0.264694   998.0         1    -0.1\n",
       "5037998 2025-02-07    847         -0.272258   999.0         1    -0.1\n",
       "5037999 2025-02-07    502         -0.361338  1000.0         1    -0.1\n",
       "\n",
       "[5038000 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:05:48.958483Z",
     "iopub.status.busy": "2025-03-30T05:05:48.958198Z",
     "iopub.status.idle": "2025-03-30T05:05:50.063799Z",
     "shell.execute_reply": "2025-03-30T05:05:50.063037Z",
     "shell.execute_reply.started": "2025-03-30T05:05:48.958461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BookValue  Traded  GrossPnL    NetPnL\n",
      "2005-02-01        1.0     1.0  0.022969  0.022869\n",
      "2005-02-02        1.0     1.4 -0.000591 -0.000731\n",
      "2005-02-03        1.0     1.4  0.018042  0.017902\n",
      "2005-02-04        1.0     1.6  0.024100  0.023940\n",
      "2005-02-07        1.0     1.4  0.023624  0.023484\n",
      "Overall Turnover (%): 142.05592543275637\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Portfolio Metrics and Backtesting\n",
    "# ---------------------------\n",
    "def calculate_metrics(df_positions, returns):\n",
    "    \"\"\"\n",
    "    Calculates daily portfolio metrics: BookValue, Traded, GrossPnL, and NetPnL.\n",
    "    \"\"\"\n",
    "    # Pivot df_positions so that each date is a row and each stock is a column.\n",
    "    pivot_weights = df_positions.pivot(index='date', columns='stock', values='weight').fillna(0.0)\n",
    "    \n",
    "    # Ensure alignment with returns.\n",
    "    common_dates = pivot_weights.index.intersection(returns.index)\n",
    "    pivot_weights = pivot_weights.loc[common_dates].sort_index()\n",
    "    returns = returns.loc[common_dates].sort_index()\n",
    "    \n",
    "    # BookValue: sum of absolute weights.\n",
    "    book_value_series = pivot_weights.abs().sum(axis=1)\n",
    "    \n",
    "    # Traded: sum of absolute differences in weights from previous day.\n",
    "    shifted_weights = pivot_weights.shift(1, fill_value=0.0)\n",
    "    traded_series = (pivot_weights - shifted_weights).abs().sum(axis=1)\n",
    "    \n",
    "    # GrossPnL: weighted sum of returns.\n",
    "    gross_pnl_series = (pivot_weights * returns).sum(axis=1)\n",
    "    \n",
    "    # NetPnL: subtract trading costs (0.01% per traded amount).\n",
    "    net_pnl_series = gross_pnl_series - (0.0001 * traded_series)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'BookValue': book_value_series,\n",
    "        'Traded': traded_series,\n",
    "        'GrossPnL': gross_pnl_series,\n",
    "        'NetPnL': net_pnl_series\n",
    "    })\n",
    "    \n",
    "    # Compute overall turnover.\n",
    "    total_traded = traded_series.sum()\n",
    "    total_book_value = book_value_series.sum()\n",
    "    turnover = (total_traded / total_book_value) * 100.0 if total_book_value != 0 else 0.0\n",
    "    \n",
    "    return metrics_df, turnover\n",
    "\n",
    "metrics_df, turnover = calculate_metrics(df_positions, returns)\n",
    "print(metrics_df.head())\n",
    "print(\"Overall Turnover (%):\", turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:05:54.159700Z",
     "iopub.status.busy": "2025-03-30T05:05:54.159410Z",
     "iopub.status.idle": "2025-03-30T05:05:54.165753Z",
     "shell.execute_reply": "2025-03-30T05:05:54.165016Z",
     "shell.execute_reply.started": "2025-03-30T05:05:54.159675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annualized Gross Sharpe Ratio: 12.3816\n",
      "Annualized Net Sharpe Ratio: 12.3205\n"
     ]
    }
   ],
   "source": [
    "def calculate_sharpes(metrics_df):\n",
    "    \"\"\"\n",
    "    Calculates Annualized Gross and Net Sharpe Ratios.\n",
    "    \"\"\"\n",
    "    gross_pnl = metrics_df['GrossPnL']\n",
    "    net_pnl   = metrics_df['NetPnL']\n",
    "    ann_factor = np.sqrt(252)\n",
    "    \n",
    "    gross_mean = gross_pnl.mean()\n",
    "    gross_std  = gross_pnl.std(ddof=1)\n",
    "    net_mean   = net_pnl.mean()\n",
    "    net_std    = net_pnl.std(ddof=1)\n",
    "    \n",
    "    gross_sharpe = ann_factor * (gross_mean / gross_std) if gross_std != 0 else np.nan\n",
    "    net_sharpe   = ann_factor * (net_mean / net_std) if net_std != 0 else np.nan\n",
    "    \n",
    "    return gross_sharpe, net_sharpe\n",
    "\n",
    "gross_sharpe, net_sharpe = calculate_sharpes(metrics_df)\n",
    "print(f\"Annualized Gross Sharpe Ratio: {gross_sharpe:.4f}\")\n",
    "print(f\"Annualized Net Sharpe Ratio: {net_sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:21:00.081784Z",
     "iopub.status.busy": "2025-03-30T05:21:00.081470Z",
     "iopub.status.idle": "2025-03-30T05:21:01.102796Z",
     "shell.execute_reply": "2025-03-30T05:21:01.102041Z",
     "shell.execute_reply.started": "2025-03-30T05:21:00.081758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission weights shape: (5038, 2167)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Prepare Final Submission Weights\n",
    "# ---------------------------\n",
    "# Get the full list of stocks from one of the feature DataFrames.\n",
    "all_stocks = features[features_list[0]].columns.tolist()\n",
    "# Pivot positions to create a DataFrame with dates as rows and stocks as columns.\n",
    "df_weights = df_positions.pivot(index='date', columns='stock', values='weight')\n",
    "# Reindex to include all stocks (fill missing with zero).\n",
    "df_weights = df_weights.reindex(columns=all_stocks, fill_value=0).fillna(0)\n",
    "print(\"Submission weights shape:\", df_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:21:01.104035Z",
     "iopub.status.busy": "2025-03-30T05:21:01.103806Z",
     "iopub.status.idle": "2025-03-30T05:21:01.206236Z",
     "shell.execute_reply": "2025-03-30T05:21:01.205502Z",
     "shell.execute_reply.started": "2025-03-30T05:21:01.104018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_dates = sorted(df_train['date'].unique())[:20]\n",
    "\n",
    "# Create a DataFrame with zero weights for these 20 dates and with the same columns as df_weights.\n",
    "df_zeros = pd.DataFrame(0.0, index=training_dates, columns=df_weights.columns)\n",
    "\n",
    "# Concatenate the zero-weight DataFrame with the original df_weights.\n",
    "# This places the 20 zero-weight days before the existing dates.\n",
    "df_weights = pd.concat([df_zeros, df_weights])\n",
    "\n",
    "# Sort the DataFrame by date to ensure proper chronological order.\n",
    "df_weights = df_weights.sort_index()\n",
    "\n",
    "df_weights.index.name = \"Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:22:52.698794Z",
     "iopub.status.busy": "2025-03-30T05:22:52.698537Z",
     "iopub.status.idle": "2025-03-30T05:22:52.705658Z",
     "shell.execute_reply": "2025-03-30T05:22:52.704952Z",
     "shell.execute_reply.started": "2025-03-30T05:22:52.698773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_weights.iloc[:2700, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:22:53.093619Z",
     "iopub.status.busy": "2025-03-30T05:22:53.093338Z",
     "iopub.status.idle": "2025-03-30T05:22:58.368207Z",
     "shell.execute_reply": "2025-03-30T05:22:58.367431Z",
     "shell.execute_reply.started": "2025-03-30T05:22:53.093597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optionally, if required, save the submission file.\n",
    "df_weights.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:22:58.369515Z",
     "iopub.status.busy": "2025-03-30T05:22:58.369307Z",
     "iopub.status.idle": "2025-03-30T05:22:58.395744Z",
     "shell.execute_reply": "2025-03-30T05:22:58.395032Z",
     "shell.execute_reply.started": "2025-03-30T05:22:58.369492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>2158</th>\n",
       "      <th>2159</th>\n",
       "      <th>2160</th>\n",
       "      <th>2161</th>\n",
       "      <th>2162</th>\n",
       "      <th>2163</th>\n",
       "      <th>2164</th>\n",
       "      <th>2165</th>\n",
       "      <th>2166</th>\n",
       "      <th>2167</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5058 rows Ã— 2167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "stock       1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "Date                                                                    ...   \n",
       "2005-01-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2005-01-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2005-01-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2005-01-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2005-01-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "2025-02-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2025-02-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2025-02-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2025-02-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2025-02-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "stock       2158  2159  2160  2161  2162  2163  2164  2165  2166  2167  \n",
       "Date                                                                    \n",
       "2005-01-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2005-01-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2005-01-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2005-01-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2005-01-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2025-02-03   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2025-02-04   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2025-02-05   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2025-02-06   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2025-02-07   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5058 rows x 2167 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11602642,
     "sourceId": 97270,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
